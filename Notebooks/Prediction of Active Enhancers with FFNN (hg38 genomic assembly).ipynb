{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from epigenomic_dataset import load_epigenomes\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from cache_decorator import Cache\n",
    "from tqdm.keras import TqdmCallback\n",
    "from barplots import barplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data retrieval\n",
    "First, we retrieve the data and impute and scale them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the data\n",
    "X, y = load_epigenomes(\n",
    "    cell_line = \"K562\",\n",
    "    dataset = \"fantom\",\n",
    "    region = \"enhancers\",\n",
    "    window_size = 256,\n",
    "    root = \"datasets\" # Path where to download data\n",
    ")\n",
    "\n",
    "# Creating the imputer\n",
    "imputer = KNNImputer()\n",
    "# Creating the scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Imputing and scaling the data\n",
    "X = pd.DataFrame(\n",
    "    scaler.fit_transform(\n",
    "        imputer.fit_transform(X)\n",
    "    ),\n",
    "    index=X.index,\n",
    "    columns=X.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "We will execute a feature selection step in each one of the holdouts, using the training data of the holdout. Otherwise, if we were to use all data, we would induce a possible positive bias in the model performance since we would use also data that we reserve to evaluate the models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "@Cache(\n",
    "    cache_path=[\n",
    "        \"active_enhancers_performance/{function_name}/kept_features_{_hash}.json\",\n",
    "        \"active_enhancers_performance/{function_name}/discarded_features_{_hash}.json\"\n",
    "    ],\n",
    "    args_to_ignore=[\n",
    "        \"X_train\", \"y_train\"\n",
    "    ]\n",
    ")\n",
    "def execute_boruta_feature_selection(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    holdout_number: int,\n",
    "    max_iter: int = 100\n",
    "):\n",
    "    \"\"\"Returns tuple with list of kept features and list of discared features.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------------\n",
    "    X_train: pd.DataFrame,\n",
    "        The data reserved for the input of the training of the Boruta model.\n",
    "    y_train: np.ndarray,\n",
    "        The data reserved for the output of the training of the Boruta model.\n",
    "    holdout_number: int,\n",
    "        The current holdout number.\n",
    "    max_iter: int = 100,\n",
    "        Number of iterations to run Boruta for.\n",
    "    \"\"\"\n",
    "    # Create the Boruta model\n",
    "    boruta_selector = BorutaPy(\n",
    "        # Defining the model that Boruta should use.\n",
    "        RandomForestClassifier(n_jobs=cpu_count(), class_weight='balanced_subsample', max_depth=5),\n",
    "        # We leave the number of estimators to be decided by Boruta\n",
    "        n_estimators='auto',\n",
    "        verbose=False,\n",
    "        alpha=0.05, # p_value\n",
    "        # In practice one would run at least 100-200 times,\n",
    "        # until all tentative features are exausted.\n",
    "        max_iter=max_iter, \n",
    "        random_state=42,\n",
    "    )\n",
    "    # Fit the Boruta model\n",
    "    boruta_selector.fit(X_train.values, y_train)\n",
    "    \n",
    "    # Get the kept features and discarded features\n",
    "    kept_features = X_train.columns[boruta_selector.support_]\n",
    "    discarded_features = X_train.columns[~boruta_selector.support_]\n",
    "    \n",
    "    # Filter out the unused featured.\n",
    "    return kept_features, discarded_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "In order to evaluate the model, we create a generator of **stratified** holdouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "number_of_splits = 10\n",
    "\n",
    "holdouts_generator = StratifiedShuffleSplit(\n",
    "    n_splits=number_of_splits,\n",
    "    test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the models predictions\n",
    "In order to evaluate the models predictions, we consider the Accuracy, AUPRC and AUROC metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from sanitize_ml_labels import sanitize_ml_labels\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score\n",
    "\n",
    "\n",
    "def evaluate_model_prediction(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Return the evaluation of the given predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ---------------------\n",
    "    y_true: np.ndarray,\n",
    "        The ground truth labels.\n",
    "    y_pred: np.ndarray,\n",
    "        The predicted labels.\n",
    "    \n",
    "    Raises\n",
    "    ---------------------\n",
    "    ValueError,\n",
    "        If the two given numpy arrays do not have the same shape.\n",
    "    \n",
    "    Returns\n",
    "    ---------------------\n",
    "    Dictionary with the performance metrics\n",
    "    \"\"\"\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\n",
    "            \"The two arrays do not have the same shape: {} != {}\".format(\n",
    "                y_true.shape, y_pred.shape\n",
    "            )\n",
    "        )\n",
    "    float_metrics = average_precision_score, roc_auc_score\n",
    "    int_metrics = (accuracy_score, )\n",
    "    return {\n",
    "        ** {\n",
    "            sanitize_ml_labels(metric.__name__): metric(y_true, y_pred)\n",
    "            for metric in float_metrics\n",
    "        },\n",
    "        ** {\n",
    "            sanitize_ml_labels(metric.__name__): metric(y_true, np.round(y_pred).astype(int))\n",
    "            for metric in int_metrics\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_model_prediction(\n",
    "    y_train_true: np.ndarray,\n",
    "    y_train_pred: np.ndarray,\n",
    "    y_test_true: np.ndarray,\n",
    "    y_test_pred: np.ndarray,\n",
    "    model_name: str,\n",
    "    holdout_number: int,\n",
    "    use_feature_selection: bool,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Return the evaluation of the given predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ---------------------\n",
    "    y_train_true: np.ndarray,\n",
    "        Ground truth labels used during training.\n",
    "    y_train_pred: np.ndarray,\n",
    "        Training predictions\n",
    "    y_test_true: np.ndarray,\n",
    "        Ground truth labels used for test.\n",
    "    y_test_pred: np.ndarray,\n",
    "        Test predictions\n",
    "    model_name: str,\n",
    "        Name of the model used.\n",
    "    holdout_number: int,\n",
    "        Number of the holdout.\n",
    "    use_feature_selection: bool,\n",
    "        Whether the model is trained using features that have\n",
    "        been selected with Boruta or not.\n",
    "    \n",
    "    Returns\n",
    "    ---------------------\n",
    "    Dictionary with the performance metrics\n",
    "    \"\"\"\n",
    "    common_informations = {\n",
    "        \"model_name\": model_name,\n",
    "        \"holdout_number\": holdout_number,\n",
    "        \"use_feature_selection\": use_feature_selection\n",
    "    }\n",
    "    return [\n",
    "        {\n",
    "            **evaluate_model_prediction(y_train_true, y_train_pred),\n",
    "            \"run_type\": \"train\",\n",
    "            **common_informations\n",
    "        },\n",
    "        {\n",
    "            **evaluate_model_prediction(y_test_true, y_test_pred),\n",
    "            \"run_type\": \"test\",\n",
    "            **common_informations\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "We define a method to create and train a Decision Tree model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "@Cache(\n",
    "    cache_path=\"active_enhancers_performance/{function_name}/{_hash}.json\",\n",
    "    args_to_ignore=[\n",
    "        \"X_train\", \"X_test\", \"y_train\", \"y_test\"\n",
    "    ]\n",
    ")\n",
    "def train_decision_tree(\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    holdout_number: int,\n",
    "    use_feature_selection: bool\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Return performance of a Decision Tree model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------------------\n",
    "    X_train: np.ndarray,\n",
    "        Data reserved for the input during training of the model.\n",
    "    X_test: np.ndarray,\n",
    "        Data reserved for the input during  test of the model.\n",
    "    y_train: np.ndarray,\n",
    "        Data reserved for the output during  training of the model.\n",
    "    y_test: np.ndarray,\n",
    "        Data reserved for the output during  test of the model.\n",
    "    holdout_number: int,\n",
    "        Number of the holdout.\n",
    "    use_feature_selection: bool,\n",
    "        Whether the model is trained using features that have\n",
    "        been selected with Boruta or not.\n",
    "        \n",
    "    Returns\n",
    "    ----------------------\n",
    "    Dictionary with the model perfomance.\n",
    "    \"\"\"\n",
    "    tree = DecisionTreeClassifier(\n",
    "        max_depth=5,\n",
    "        min_samples_leaf=100\n",
    "    )\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_train_pred = tree.predict(X_train)\n",
    "    y_test_pred = tree.predict(X_test)\n",
    "    return evaluate_all_model_prediction(\n",
    "        y_train, y_train_pred, y_test, y_test_pred,\n",
    "        \"Decision Tree\", holdout_number, use_feature_selection\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "We define a method to create and train a Random Forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "@Cache(\n",
    "    cache_path=\"active_enhancers_performance/{function_name}/{_hash}.json\",\n",
    "    args_to_ignore=[\n",
    "        \"X_train\", \"X_test\", \"y_train\", \"y_test\"\n",
    "    ]\n",
    ")\n",
    "def train_random_forest(\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    holdout_number: int,\n",
    "    use_feature_selection: bool\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Return performance of a Random Forest model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------------------\n",
    "    X_train: np.ndarray,\n",
    "        Data reserved for the input during training of the model.\n",
    "    X_test: np.ndarray,\n",
    "        Data reserved for the input during  test of the model.\n",
    "    y_train: np.ndarray,\n",
    "        Data reserved for the output during  training of the model.\n",
    "    y_test: np.ndarray,\n",
    "        Data reserved for the output during  test of the model.\n",
    "    holdout_number: int,\n",
    "        Number of the holdout.\n",
    "    use_feature_selection: bool,\n",
    "        Whether the model is trained using features that have\n",
    "        been selected with Boruta or not.\n",
    "        \n",
    "    Returns\n",
    "    ----------------------\n",
    "    Dictionary with the model perfomance.\n",
    "    \"\"\"\n",
    "    forest = RandomForestClassifier(\n",
    "        n_estimators=600,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        max_depth=5,\n",
    "        min_samples_leaf=100,\n",
    "        n_jobs=cpu_count(),\n",
    "        verbose=False\n",
    "    )\n",
    "    forest.fit(X_train, y_train)\n",
    "    y_train_pred = forest.predict(X_train)\n",
    "    y_test_pred = forest.predict(X_test)\n",
    "    return evaluate_all_model_prediction(\n",
    "        y_train, y_train_pred, y_test, y_test_pred,\n",
    "        \"Random Forest\", holdout_number, use_feature_selection\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "We define a method to create and train a Perceptron model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from extra_keras_metrics import get_standard_binary_metrics\n",
    "\n",
    "\n",
    "@Cache(\n",
    "    cache_path=\"active_enhancers_performance/{function_name}/{_hash}.json\",\n",
    "    args_to_ignore=[\n",
    "        \"X_train\", \"X_test\", \"y_train\", \"y_test\"\n",
    "    ]\n",
    ")\n",
    "def train_perceptron(\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    holdout_number: int,\n",
    "    use_feature_selection: bool\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Return performance of a Perceptron.\n",
    "    \n",
    "    Parameters\n",
    "    ----------------------\n",
    "    X_train: np.ndarray,\n",
    "        Data reserved for the input during training of the model.\n",
    "    X_test: np.ndarray,\n",
    "        Data reserved for the input during  test of the model.\n",
    "    y_train: np.ndarray,\n",
    "        Data reserved for the output during  training of the model.\n",
    "    y_test: np.ndarray,\n",
    "        Data reserved for the output during  test of the model.\n",
    "    holdout_number: int,\n",
    "        Number of the holdout.\n",
    "    use_feature_selection: bool,\n",
    "        Whether the model is trained using features that have\n",
    "        been selected with Boruta or not.\n",
    "        \n",
    "    Returns\n",
    "    ----------------------\n",
    "    Dictionary with the model perfomance.\n",
    "    \"\"\"\n",
    "    perceptron = Sequential([\n",
    "        Input((X_train.shape[1], )),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    perceptron.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=\"nadam\",\n",
    "        metrics=get_standard_binary_metrics()\n",
    "    )\n",
    "    perceptron.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=1000,\n",
    "        batch_size=1024,\n",
    "        verbose=False,\n",
    "        callbacks=[\n",
    "            EarlyStopping(\"loss\"),\n",
    "            # I have commented this because we do not need this loading bar\n",
    "            # when running the main experiment loop. When you experiment with\n",
    "            # the model structure you may want to enable this to get a feel\n",
    "            # of how the model is performing during the training.\n",
    "            #TqdmCallback(verbose=1)\n",
    "        ]\n",
    "    )\n",
    "    y_train_pred = perceptron.predict(X_train)\n",
    "    y_test_pred = perceptron.predict(X_test)\n",
    "    return evaluate_all_model_prediction(\n",
    "        y_train, y_train_pred, y_test, y_test_pred,\n",
    "        \"Perceptron\", holdout_number, use_feature_selection\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFNN\n",
    "We define a method to create and train a FFNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from extra_keras_metrics import get_standard_binary_metrics\n",
    "\n",
    "\n",
    "@Cache(\n",
    "    cache_path=\"active_enhancers_performance/{function_name}/{_hash}.json\",\n",
    "    args_to_ignore=[\n",
    "        \"X_train\", \"X_test\", \"y_train\", \"y_test\"\n",
    "    ]\n",
    ")\n",
    "def train_ffnn(\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    holdout_number: int,\n",
    "    use_feature_selection: bool\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Return performance of a FFNN.\n",
    "    \n",
    "    Parameters\n",
    "    ----------------------\n",
    "    X_train: np.ndarray,\n",
    "        Data reserved for the input during training of the model.\n",
    "    X_test: np.ndarray,\n",
    "        Data reserved for the input during  test of the model.\n",
    "    y_train: np.ndarray,\n",
    "        Data reserved for the output during  training of the model.\n",
    "    y_test: np.ndarray,\n",
    "        Data reserved for the output during  test of the model.\n",
    "    holdout_number: int,\n",
    "        Number of the holdout.\n",
    "    use_feature_selection: bool,\n",
    "        Whether the model is trained using features that have\n",
    "        been selected with Boruta or not.\n",
    "        \n",
    "    Returns\n",
    "    ----------------------\n",
    "    Dictionary with the model perfomance.\n",
    "    \"\"\"\n",
    "    ffnn = Sequential([\n",
    "        Input((X_train.shape[1], )),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    ffnn.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=\"nadam\",\n",
    "        metrics=get_standard_binary_metrics()\n",
    "    )\n",
    "    ffnn.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=1000,\n",
    "        batch_size=1024,\n",
    "        verbose=False,\n",
    "        callbacks=[\n",
    "            EarlyStopping(\"loss\"),\n",
    "            # I have commented this because we do not need this loading bar\n",
    "            # when running the main experiment loop. When you experiment with\n",
    "            # the model structure you may want to enable this to get a feel\n",
    "            # of how the model is performing during the training.\n",
    "            # TqdmCallback(verbose=1)\n",
    "        ]\n",
    "    )\n",
    "    y_train_pred = ffnn.predict(X_train)\n",
    "    y_test_pred = ffnn.predict(X_test)\n",
    "    return evaluate_all_model_prediction(\n",
    "        y_train, y_train_pred, y_test, y_test_pred,\n",
    "        \"FFNN\", holdout_number, use_feature_selection\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we create the main loop!\n",
    "Now we can put everything togheter and run our experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c99024f68b42c38083e8f3612e1ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing holdouts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81b9f4bef5d4f078b0e202042c29389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running feature selection:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a list to store all the computed performance\n",
    "all_performance = []\n",
    "\n",
    "# Start the main loop, iterating through the holdouts\n",
    "for holdout_number, (train_indices, test_indices) in tqdm(\n",
    "    enumerate(holdouts_generator.split(X, y)),\n",
    "    total=number_of_splits,\n",
    "    desc=\"Computing holdouts\"\n",
    "):\n",
    "    # We do an internal loop for whether to use feature selection or not\n",
    "    # Generally speaking, we always want to know if feature selection is doing something or not.\n",
    "    for use_feature_selection in tqdm((True, False), desc=\"Running feature selection\", leave=False):\n",
    "        X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\n",
    "        y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]\n",
    "        # If the flag for feature selection is set, we compute the features\n",
    "        # To keep and discard using Boruta.\n",
    "        # We only do the feature selection once per holdout\n",
    "        # because the data remains constant within the holdout.\n",
    "        if use_feature_selection:\n",
    "            kept_features, discarded_features = execute_boruta_feature_selection(\n",
    "                X_train,\n",
    "                y_train.values.ravel(),\n",
    "                holdout_number\n",
    "            )\n",
    "            # We filter the DataFrames columns using the features that\n",
    "            # Boruta has decided we should keep\n",
    "            X_train = X_train[kept_features]\n",
    "            X_test = X_test[kept_features]\n",
    "        # We train the models we have prepared, that is:\n",
    "        # - A Decision Tree\n",
    "        # - A Random Forest\n",
    "        # - A Perceptron\n",
    "        # - A Feed-Forward Neural-Network (FFNN)\n",
    "        for train_model in tqdm(\n",
    "            (train_decision_tree, train_random_forest, train_perceptron, train_ffnn),\n",
    "            desc=\"Training models\",\n",
    "            leave=False\n",
    "        ):\n",
    "            # We compute the model performance\n",
    "            performance = train_model(\n",
    "                X_train.values, X_test.values, y_train.values, y_test.values,\n",
    "                holdout_number,\n",
    "                use_feature_selection\n",
    "            )\n",
    "            # We chain the computed performance to the performance list\n",
    "            all_performance += performance\n",
    "    \n",
    "# We convert the computed performance list into a DataFrame\n",
    "all_performance = pd.DataFrame(all_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results visualization\n",
    "Now that we have run our experiment we can visualize its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "barplots(\n",
    "    all_performance,\n",
    "    groupby=[\"model_name\", \"use_feature_selection\", \"run_type\"],\n",
    "    orientation=\"horizontal\",\n",
    "    height=4\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
